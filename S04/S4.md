# Lecturer Notes: Applied High Performance Computing

"""
Fundamentos en HPC:
- Comunicación y partición de datos
"""

### Resumen:
- Uso de cluster computacional (Khipu)
- Protocolo MPI y comunicación entre procesos

### Unidad 4.1: MPI y comunicación entre procesos

---------------
**Message Passing Interface (MPI):**
- Protocolo de comunicación entre procesos, con memoria local.
- Implica el envío/recibo de mensajes.
- Independiente de la topología usada.

Características:
- Espacio de memoria particionado en p nodos.
- Funciona en CPU, Nodo, Cluster (paralelismo explícito).
- Compatible con NUMA (Non-uniform memory access): rápido acceso local.

Implementaciones:
- MPICH (estable: mpich-4.0.2)
- OPENMPI (estable: openmpi-4.1.3)

Extensiones (bindings):
- Julia
- MATLAB
- Python
- R
(Puede combinarse con OpenMP)
"""
------------

### Message Passing Interface
Estructura de un programa en MPI:

```
#python
from mpi4py import MP
```

Cada proceso tiene un número entero (rank) comenzando desde 0.

Ejecución de código en 4 procesos:
mpiexec -n 4 python3 code.py

Referencia:
https://mpi4py.readthedocs.io/en/stable/index.html

---------------
### Message Passing Interface for Python:  mpi4py

"""
- Wrapper de Python para el estándar MPI.
- Permite establecer comunicación entre procesos/nodos.

Aplicaciones:
**Machine Learning & Data Science:**
- Entrenamiento distribuido (Horovod: TensorFlow, PyTorch)
- Data Processing Pipelines (ETL, Big Data en genómica, astrofísica)

**Computación científica:**
    - CFD (PyFR)
    - Dinámica molecular (HOOMD-blue)
    - Química cuántica (PySCF)

Otras áreas:
- Modelado climático (FRE-NC)
- Sismología
- Bioinformática (análisis de genomas)
- Rendering y procesamiento de imágenes (ray tracing)
"""
---------
### Tipos de comunicación en MPI


Comunicación punto a punto:

comm.send(buf, dest, tag=0)
comm.recv(buf=None, source=ANY_SOURCE, tag=ANY_TAG, status=None)

Comunicación colectiva:

Broadcast:
comm.bcast(obj, root=0)

Scatter:
comm.scatter(sendobj, root=0)

Gather:
comm.gather(sendobj, root=0)

Reduce:
comm.reduce(sendobj, op=MPI.SUM, root=0)
"""

# Caso 4.1: Escalabilidad en CPU local

"""
Caso 1: Cálculo paralelo de PI

- Usar código pi.skel.py (/mpi4py-ejemplos).
- Analizar escalabilidad:
    - Intervalos de n = 2^10 a 2^24.
    - Procesos: p = 2, 4, 8, 16 (y opcionales intermedios).
- Medir tiempos de ejecución vs cantidad de procesos.
- Graficar T_p (tiempo), Speedup y Eficiencia.

Análisis:
- Escalabilidad fuerte y débil.

Preguntas:
- ¿Cantidad óptima de procesos?
- ¿Mayor speedup implica mayor eficiencia?
- ¿Qué métrica priorizar?
"""

# Unidad 4.2: Uso de cluster computacional (Khipu)

"""
Pasos:
1. Ingresar a cuenta en Khipu.
2. Copiar datos (código, data) a una carpeta en el cluster.

Envío de trabajos:
- Crear un job script usando SLURM.
- Documentación:
    - https://docs.khipu.utec.edu.pe/guia-de-usuario/ejemplos/python/
    - https://docs.khipu.utec.edu.pe/guia-de-usuario/comandos-basicos/?h=slurm

Post-procesamiento:
- Copiar la data resultante a la computadora local.
"""

# Caso 4.2: Escalabilidad en cluster computacional

"""
Caso 2: Uso de cluster computacional

- Enviar el job script del caso anterior a Khipu.
- Simular los mismos experimentos, pero con más procesos y datos.
- Comparar escalabilidad respecto a CPU local.
- Preparar resultados para discusión en clase.
"""

# Resumen:

"""
- Principios de distribución de datos usando MPI.
- Evaluación de escalabilidad en CPU local y cluster computacional (Khipu).
"""

# Bibliografía:

"""
- MPI for Python (mpi4py): https://mpi4py.readthedocs.io/en/stable/index.html
- Cluster Khipu (UTEC): https://docs.khipu.utec.edu.pe/
- Quinn, M. J. (2003). Parallel Programming in C with MPI and OpenMP. McGraw-Hill Education Group (Capítulo 4).
"""
